import os
import sys
import logging
import uuid
from typing import List
from pathlib import Path
from dotenv import load_dotenv

# è¨­å®šè·¯å¾‘
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

import qdrant_client
from qdrant_client import models
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from src.ingestion.schema import ProcessedChunk

load_dotenv()
logger = logging.getLogger(__name__)

class VectorIndexer:
    def __init__(self, collection_name: str = "rag_knowledge_base"):
        self.collection_name = collection_name
        self.client = qdrant_client.QdrantClient(url=os.getenv("QDRANT_URL"))
        
        # ä½¿ç”¨ OpenAI Embedding æ¨¡å‹ (å°æ‡‰ Roadmap Source 6)
        # text-embedding-3-small æ€§åƒ¹æ¯”é«˜ï¼Œé©åˆ MVP
        self.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
        
        # åˆå§‹åŒ– Collection
        self._init_collection()

    def _init_collection(self):
        """æª¢æŸ¥ä¸¦å»ºç«‹ Qdrant Collectionï¼Œè¨­å®š Named Vectors"""
        if not self.client.collection_exists(self.collection_name):
            logger.info(f"ğŸ”¨ æ­£åœ¨å»ºç«‹å‘é‡é›†åˆ: {self.collection_name}")
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config={
                    # 1. å…§å®¹å‘é‡ (Content Vector)
                    "content": models.VectorParams(size=1536, distance=models.Distance.COSINE),
                    # 2. å•é¡Œå‘é‡ (Question Vector) - é€™æ˜¯ NQ1D çš„é—œéµ
                    "question": models.VectorParams(size=1536, distance=models.Distance.COSINE),
                }
            )
        else:
            logger.info(f"âœ… å‘é‡é›†åˆå·²å­˜åœ¨: {self.collection_name}")

    def index(self, chunks: List[ProcessedChunk]):
        """
        å°‡è™•ç†å¥½çš„ Chunks å‘é‡åŒ–ä¸¦å¯«å…¥ Qdrant
        """
        points = []
        logger.info(f"âš¡ æ­£åœ¨ç‚º {len(chunks)} ç­†è³‡æ–™ç”Ÿæˆå‘é‡...")

        for chunk in chunks:
            try:
                # 1. ç”Ÿæˆ Content Vector (é‡å°åŸå§‹æ–‡æœ¬)
                vec_content = self.embed_model.get_text_embedding(chunk.text)
                
                # 2. ç”Ÿæˆ Question Vector (é‡å° NQ1D) 
                # å–ç¬¬ä¸€å€‹ canonical_q ä½œç‚ºä¸»è¦ç´¢å¼•
                if chunk.semantic_data.nq1d:
                    q_text = chunk.semantic_data.nq1d[0].canonical_q
                    vec_question = self.embed_model.get_text_embedding(q_text)
                else:
                    # å¦‚æœæ²’æœ‰å•é¡Œï¼Œå°±ç”¨ content è£œä½ (é¿å…å ±éŒ¯)
                    vec_question = vec_content

                # 3. æº–å‚™ Payload (Metadata) 
                payload = {
                    "file_name": chunk.file_name,
                    "page_label": chunk.page_label,
                    "text": chunk.text,
                    "summary": chunk.semantic_data.summary,
                    "what": chunk.semantic_data.what,
                    "why": chunk.semantic_data.why,
                    "how": json.dumps(chunk.semantic_data.how, ensure_ascii=False), # è½‰å­—ä¸²å­˜
                    "canonical_q": chunk.semantic_data.nq1d[0].canonical_q if chunk.semantic_data.nq1d else "",
                    "keywords": chunk.semantic_data.keywords
                }

                # 4. å»ºç«‹ Qdrant Point
                points.append(models.PointStruct(
                    id=str(uuid.uuid4()), # éš¨æ©Ÿç”Ÿæˆ ID
                    vector={
                        "content": vec_content,
                        "question": vec_question
                    },
                    payload=payload
                ))
            except Exception as e:
                logger.error(f"âŒ å‘é‡åŒ–å¤±æ•— Chunk {chunk.chunk_id}: {e}")

        # 5. æ‰¹æ¬¡å¯«å…¥
        if points:
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            logger.info(f"âœ… æˆåŠŸå¯«å…¥ {len(points)} ç­†è³‡æ–™åˆ° Qdrantï¼")
        else:
            logger.warning("âš ï¸ æ²’æœ‰è³‡æ–™è¢«å¯«å…¥ã€‚")

# å–®å…ƒæ¸¬è©¦
if __name__ == "__main__":
    import json
    # æ¨¡æ“¬ä¸€å€‹ ProcessedChunk ä¾†æ¸¬è©¦ (ä¸ç”¨æ¯æ¬¡éƒ½è·‘ LLM ç‡’éŒ¢)
    from src.ingestion.schema import SemanticExtraction, NQ1DItem
    
    logging.basicConfig(level=logging.INFO)

    # é€ å‡è³‡æ–™
    mock_data = SemanticExtraction(
        summary="æ¸¬è©¦æ‘˜è¦",
        what="æ¸¬è©¦å®šç¾©",
        why="æ¸¬è©¦åŸå› ",
        how=["æ­¥é©Ÿ1", "æ­¥é©Ÿ2"],
        nq1d=[NQ1DItem(canonical_q="é€™æ˜¯æ¸¬è©¦å•é¡Œå—ï¼Ÿ", intent="test")],
        keywords=["test", "mock"]
    )
    
    mock_chunk = ProcessedChunk(
        chunk_id="test_001",
        file_name="test_doc.pdf",
        page_label="1",
        text="é€™æ˜¯ä¸€æ®µæ¸¬è©¦æ–‡å­—ï¼Œç”¨æ–¼é©—è­‰å‘é‡å¯«å…¥æ˜¯å¦æˆåŠŸã€‚",
        semantic_data=mock_data
    )

    indexer = VectorIndexer()
    indexer.index([mock_chunk])