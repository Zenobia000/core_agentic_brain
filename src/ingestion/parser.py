import os
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv

# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„ (ç¢ºä¿èƒ½è®€åˆ° .env èˆ‡ data)
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

from llama_index.core import SimpleDirectoryReader, Settings
from llama_index.core.node_parser import SentenceSplitter

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def load_and_chunk_documents(data_dir: str = "data/raw"):
    """
    1. è®€å–æŒ‡å®šç›®éŒ„ä¸‹çš„ PDF/MD æ–‡ä»¶
    2. åŸ·è¡Œåˆæ­¥åˆ‡åˆ† (Structure Pass)
    3. è¿”å› Node åˆ—è¡¨
    """
    input_dir = BASE_DIR / data_dir
    
    if not input_dir.exists():
        logger.error(f"âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
        return []

    # 1. è®€å–æ–‡ä»¶ (Ingestion)
    logger.info(f"ğŸ“‚ é–‹å§‹æƒæç›®éŒ„: {input_dir} ...")
    reader = SimpleDirectoryReader(
        input_dir=str(input_dir),
        recursive=True,
        required_exts=[".pdf", ".md"], # é–å®š PDF èˆ‡ Markdown
        filename_as_id=True
    )
    documents = reader.load_data()
    logger.info(f"âœ… æˆåŠŸè®€å– {len(documents)} é åŸå§‹æ–‡ä»¶")

    if not documents:
        logger.warning("âš ï¸ ç›®éŒ„ç‚ºç©ºï¼Œè«‹æ”¾å…¥ .pdf æª”æ¡ˆï¼")
        return []

    # 2. åˆ‡åˆ†ç­–ç•¥ (Chunking Strategy) - å°æ‡‰ Roadmap 1.1
    # é€™è£¡ä½¿ç”¨ SentenceSplitter åšç‚ºåŸºç¤åˆ‡åˆ† (Structure Pass)
    # chunk_size=1024 ç´„å°æ‡‰ä¸­æ–‡ 500-800 å­—ï¼Œä¿ç•™ä¸Šä¸‹æ–‡
    splitter = SentenceSplitter(
        chunk_size=1024,
        chunk_overlap=200
    )

    nodes = splitter.get_nodes_from_documents(documents)
    
    # 3. è£œå…… Metadata (ç‚º Phase 1.2 èªæ„èƒå–åšæº–å‚™)
    for node in nodes:
        # ç¢ºä¿æœ‰æª”åè³‡è¨Šï¼Œæ–¹ä¾¿å¾ŒçºŒå›æº¯
        file_name = node.metadata.get("file_name", "unknown")
        page_label = node.metadata.get("page_label", "1")
        
        # æ‚¨å¯ä»¥åœ¨é€™è£¡åŠ å…¥æ›´å¤šè‡ªå®šç¾© Metadata é‚è¼¯
        node.metadata["processed_by"] = "parser_v1"
        
        # ç°¡åŒ–é¡¯ç¤ºç”¨
        logger.debug(f"Chunk created: {file_name} (Page {page_label}) - {len(node.text)} chars")

    logger.info(f"âœ‚ï¸  æ–‡ä»¶å·²åˆ‡åˆ†ç‚º {len(nodes)} å€‹ç¯€é» (Chunks)")
    return nodes

if __name__ == "__main__":
    # æ¸¬è©¦åŸ·è¡Œ
    try:
        nodes = load_and_chunk_documents()
        if nodes:
            # é è¦½ç¬¬ä¸€å€‹ Chunk çš„å…§å®¹
            print("\n" + "="*50)
            print(f"ğŸ‘€ é è¦½ç¬¬ä¸€å€‹ Chunk (ä¾†è‡ª: {nodes[0].metadata['file_name']})")
            print("-" * 50)
            print(nodes[0].text[:500] + "...") # åªå°å‰ 500 å­—
            print("="*50 + "\n")
            print(f"âœ… Parser æ¸¬è©¦æˆåŠŸï¼æº–å‚™é€²å…¥èªæ„èƒå– (Phase 1.2)")
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")