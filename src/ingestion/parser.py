import os
import sys
import logging
from pathlib import Path
from dotenv import load_dotenv

# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

from llama_index.core import SimpleDirectoryReader
# 1. å¼•å…¥èªæ„åˆ‡åˆ†å™¨
from llama_index.core.node_parser import SemanticSplitterNodeParser
# 2. å¼•å…¥ Docling Reader
from llama_index.readers.docling import DoclingReader
# 3. å¼•å…¥ Cohere Embedding (ä½œç‚ºåˆ‡åˆ†ä¾æ“š)
from llama_index.embeddings.cohere import CohereEmbedding

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def load_and_chunk_documents(data_dir: str = "data/raw"):
    """
    ã€çµ‚æ¥µæ··åˆç­–ç•¥ã€‘
    1. Parsing: ä½¿ç”¨ IBM Docling å°‡ PDF è½‰ç‚ºä¹¾æ·¨çš„ Markdown (è§£æ±ºæ’ç‰ˆ/è¡¨æ ¼å•é¡Œ)
    2. Chunking: ä½¿ç”¨ Cohere Embedding é€²è¡Œèªæ„åˆ‡åˆ† (è§£æ±ºä¸Šä¸‹æ–‡æˆªæ–·å•é¡Œ)
    """
    input_dir = BASE_DIR / data_dir
    
    if not input_dir.exists():
        logger.error(f"âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
        return []

    # --- Step 1: Docling è§£æ (æ¸…æ´—è³‡æ–™) ---
    logger.info("ğŸ§  [Step 1] åˆå§‹åŒ– Docling è§£æå™¨ (Layout Parsing)...")
    docling_reader = DoclingReader(export_type="markdown")

    reader = SimpleDirectoryReader(
        input_dir=str(input_dir),
        recursive=True,
        required_exts=[".pdf"],
        file_extractor={".pdf": docling_reader},
        filename_as_id=True
    )
    
    logger.info("ğŸš€ æ­£åœ¨åŸ·è¡Œ Docling è§£æ (é€™æœƒèŠ±ä¸€é»æ™‚é–“)...")
    # é€™è£¡å‡ºä¾†çš„æ˜¯æ•´ä»½å®Œæ•´çš„æ–‡ä»¶ï¼Œé‚„æ²’åˆ‡
    documents = reader.load_data()
    logger.info(f"âœ… Docling æ¸…æ´—å®Œæˆï¼ç²å¾— {len(documents)} ä»½çµæ§‹åŒ–æ–‡ä»¶")

    # --- Step 2: Cohere èªæ„åˆ‡åˆ† (ç²¾æº–ä¸‹åˆ€) ---
    logger.info("ğŸ§  [Step 2] åˆå§‹åŒ– Cohere èªæ„åˆ‡åˆ†å™¨ (Semantic Chunking)...")
    
    # ä½¿ç”¨ä½ çš„ Cohere Key
    api_key = os.getenv("COHERE_API_KEY")
    if not api_key:
        logger.error("âŒ æ‰¾ä¸åˆ° COHERE_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
        return []

    embed_model = CohereEmbedding(
        cohere_api_key=api_key,
        model_name="embed-multilingual-v3.0", # æ”¯æ´ä¸­æ–‡æœ€å¼·
        input_type="search_document"
    )
    
    # è¨­å®šåˆ‡åˆ†å™¨
    # buffer_size=1: æ¯”è¼ƒå‰å¾Œå¥
    # breakpoint_percentile_threshold=95: åªæœ‰èªæ„å·®ç•°æ¥µå¤§æ™‚æ‰åˆ‡æ–· (ä¿æŒæ®µè½å®Œæ•´æ€§)
    splitter = SemanticSplitterNodeParser(
        buffer_size=1,
        breakpoint_percentile_threshold=95, 
        embed_model=embed_model
    )

    logger.info("âœ‚ï¸  æ­£åœ¨ä½¿ç”¨ Cohere è¨ˆç®—èªæ„è·é›¢ä¸¦åˆ‡åˆ†...")
    # é€™è£¡æœƒçœŸçš„å¾ˆæ…¢ï¼Œå› ç‚ºæ¯ä¸€å¥éƒ½è¦ call API
    # ç‚ºäº†é˜²æ­¢ Trial Key çˆ†æ‰ï¼Œæˆ‘å€‘é€™è£¡ä¸ç‰¹åˆ¥åŠ  sleepï¼Œä½†å¦‚æœæ–‡ä»¶å¤ªå¤§å¯èƒ½æœƒ 429
    nodes = splitter.get_nodes_from_documents(documents)
    
    # 3. è£œå…… Metadata
    for node in nodes:
        node.metadata["processed_by"] = "docling_plus_cohere_semantic"
        logger.debug(f"Hybrid Chunk: {len(node.text)} chars")

    logger.info(f"âœ… æ··åˆç­–ç•¥åˆ‡åˆ†å®Œæˆï¼å…±ç”Ÿæˆ {len(nodes)} å€‹èªæ„ç¯€é»")
    
    return nodes

if __name__ == "__main__":
    try:
        nodes = load_and_chunk_documents()
        if nodes:
            print("\n" + "="*50)
            print(f"ğŸ‘€ é è¦½ Docling + Cohere åˆ‡åˆ†çµæœ:")
            print("-" * 50)
            # çœ‹çœ‹åˆ‡å‡ºä¾†çš„ç¬¬ä¸€æ®µé•·ä»€éº¼æ¨£
            print(nodes[0].text[:800] + "...") 
            print("="*50)
            
            # é©—è­‰ä¸€ä¸‹æ˜¯ä¸æ˜¯çœŸçš„ç…§èªæ„åˆ‡ (é•·åº¦æ‡‰è©²å¾ˆä¸å›ºå®š)
            print("ğŸ“Š Chunk é•·åº¦åˆ†ä½ˆ (å‰ 5 å€‹):")
            for i, n in enumerate(nodes[:5]):
                print(f"   Chunk {i+1}: {len(n.text)} chars")
                
            print(f"\nâœ… Parser æ¸¬è©¦æˆåŠŸï¼")
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")