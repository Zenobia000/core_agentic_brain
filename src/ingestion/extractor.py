import os
import sys
import json
import logging
from pathlib import Path
from dotenv import load_dotenv

# --- [é—œéµä¿®æ­£] è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„è·¯å¾‘ï¼Œç¢ºä¿èƒ½ Import src æ¨¡çµ„ ---
# å–å¾—ç•¶å‰æª”æ¡ˆçš„ä¸Šä¸€å±¤çš„ä¸Šä¸€å±¤ (å³å°ˆæ¡ˆæ ¹ç›®éŒ„ rag-project)
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))
# -----------------------------------------------------------

from llama_index.core import Document
from llama_index.core.schema import TextNode
from llama_index.llms.openai import OpenAI
from llama_index.core.prompts import PromptTemplate

# ç¾åœ¨ Python æ‰¾å¾—åˆ° src äº†
from src.ingestion.schema import SemanticExtraction, ProcessedChunk

load_dotenv()
logger = logging.getLogger(__name__)

# --- System Prompt è¨­è¨ˆ (Roadmap Phase 1.2) ---
EXTRACT_PROMPT_TMPL = """
ä½ æ˜¯ä¸€ä½è³‡æ·±çš„æŠ€è¡“æ–‡ä»¶åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯å¾ä»¥ä¸‹ã€Œæ–‡ä»¶ç‰‡æ®µ (Chunk)ã€ä¸­èƒå–é—œéµçŸ¥è­˜ï¼Œä¸¦å°‡å…¶è½‰åŒ–ç‚ºçµæ§‹åŒ–æ•¸æ“šã€‚

è«‹ç‰¹åˆ¥é—œæ³¨ï¼š
1. **WHAT**: é€™æ®µæ–‡å­—åœ¨è¬›ä»€éº¼æ ¸å¿ƒæ¦‚å¿µï¼Ÿ
2. **WHY**: ç‚ºä»€éº¼é€™æ¨£åšï¼Ÿæœ‰ä»€éº¼å¥½è™•æˆ–åŸå› ï¼Ÿ
3. **HOW**: å…·é«”çš„æ–¹æ³•ã€æ­¥é©Ÿæˆ–æ¼”ç®—æ³•ç´°ç¯€ã€‚
4. **NQ1D**: æƒ³åƒä½¿ç”¨è€…æœƒå•ä»€éº¼å•é¡Œï¼Œè€Œé€™æ®µæ–‡å­—æ­£å¥½æ˜¯å®Œç¾ç­”æ¡ˆï¼Ÿè«‹ç”Ÿæˆ "Canonical Question" (æ¨™æº–åŒ–å•é¡Œ)ã€‚

æ–‡ä»¶ç‰‡æ®µå…§å®¹ï¼š
---------------------
{context_str}
---------------------

è«‹ä»¥ç¹é«”ä¸­æ–‡è¼¸å‡ºï¼Œä¸¦åš´æ ¼éµå®ˆ JSON Schema æ ¼å¼ã€‚
å¦‚æœè©²ç‰‡æ®µæ²’æœ‰åŒ…å«ç‰¹å®šæ¬„ä½ï¼ˆå¦‚æ²’æœ‰æ­¥é©Ÿï¼‰ï¼Œè«‹åœ¨è©²æ¬„ä½å¡«å…¥ "N/A" æˆ–ç©ºé™£åˆ—ã€‚
"""

class SemanticExtractor:
    def __init__(self):
        # ä½¿ç”¨ GPT-4o ç¢ºä¿ JSON éµå¾ªèƒ½åŠ›èˆ‡æ¨ç†è§£æèƒ½åŠ›
        self.llm = OpenAI(model="gpt-4o", temperature=0.1)
        self.prompt = PromptTemplate(EXTRACT_PROMPT_TMPL)

    def extract(self, node: TextNode) -> ProcessedChunk:
        """
        å°å–®ä¸€ Node é€²è¡Œ LLM èƒå–
        """
        try:
            # 1. æ§‹å»º Prompt
            fmt_prompt = self.prompt.format(context_str=node.text)
            
            # 2. å‘¼å« LLM (ä½¿ç”¨ structured_predict å¼·åˆ¶è¼¸å‡º Pydantic æ ¼å¼)
            extraction = self.llm.structured_predict(
                SemanticExtraction, 
                prompt=self.prompt,
                context_str=node.text
            )
            
            # 3. çµ„è£æœ€çµ‚ç‰©ä»¶
            processed_chunk = ProcessedChunk(
                chunk_id=node.node_id,
                file_name=node.metadata.get("file_name", "unknown"),
                page_label=node.metadata.get("page_label", "unknown"),
                text=node.text,
                semantic_data=extraction
            )
            
            logger.info(f"âœ… Extracted: {processed_chunk.file_name} (Page {processed_chunk.page_label}) - Q: {extraction.nq1d[0].canonical_q}")
            return processed_chunk

        except Exception as e:
            logger.error(f"âŒ Extraction failed for node {node.node_id}: {e}")
            return None

# å–®å…ƒæ¸¬è©¦å€
if __name__ == "__main__":
    # é€™è£¡ä¸éœ€è¦å† append path äº†ï¼Œå› ç‚ºä¸Šé¢å·²ç¶“åšéäº†
    from src.ingestion.parser import load_and_chunk_documents
    
    logging.basicConfig(level=logging.INFO)
    
    # 1. è®€å–æ–‡ä»¶
    print("ğŸ“‚ æ­£åœ¨è¼‰å…¥æ–‡ä»¶ä¸¦åˆ‡å¡Š...")
    nodes = load_and_chunk_documents()
    
    if nodes:
        extractor = SemanticExtractor()
        # æ¸¬è©¦ï¼šåªè·‘ç¬¬ 2 å€‹ Chunk (é¿é–‹å°é¢)
        target_idx = 1 if len(nodes) > 1 else 0
        target_node = nodes[target_idx]
        
        print(f"\nğŸ¤– æ­£åœ¨å° Chunk {target_idx} é€²è¡Œ AI èƒå– (Text: {target_node.text[:50]}...)\n")
        result = extractor.extract(target_node)
        
        if result:
            print("\n" + "="*50)
            print("ğŸš€ èƒå–çµæœ (JSON):")
            print(result.semantic_data.model_dump_json(indent=2))
            print("="*50 + "\n")