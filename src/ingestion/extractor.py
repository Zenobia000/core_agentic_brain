import os
import logging
import asyncio
from typing import List
from llama_index.core.schema import BaseNode
from openai import AsyncOpenAI

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def extract_nq1d(nodes: List[BaseNode]) -> List[BaseNode]:
    """
    ä½¿ç”¨ LLM (GPT-4o) ç‚ºæ¯ä¸€å€‹ Chunk ç”Ÿæˆã€Œæ¨™æº–åŒ–å•é¡Œ (NQ1D)ã€ã€‚
    é€™äº›å•é¡Œå°‡è¢«å­˜å…¥ node.metadata["questions"]ï¼Œç”¨æ–¼å¾ŒçºŒçš„ç²¾æº–æª¢ç´¢ã€‚
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("âŒ æœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•åŸ·è¡Œèªæ„èƒå–")
        return nodes

    client = AsyncOpenAI(api_key=api_key)
    
    logger.info(f"ğŸ¤– æ­£åœ¨ç‚º {len(nodes)} å€‹ç¯€é»ç”Ÿæˆ NQ1D å•é¡Œ...")

    # å®šç¾©è™•ç†å–®å€‹ç¯€é»çš„å‡½æ•¸ (åŒ…å«é‡è©¦æ©Ÿåˆ¶)
    async def process_node(node: BaseNode, index: int):
        # ç°¡å–®çš„é˜²å‘†ï¼šå¦‚æœå…§å®¹å¤ªçŸ­ï¼Œå°±ä¸ç”Ÿæˆå•é¡Œäº†
        if len(node.text) < 50:
            node.metadata["questions"] = []
            return

        prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è³‡æ–™åˆ†æå¸«ã€‚è«‹é–±è®€ä»¥ä¸‹æŠ€è¡“æ–‡ä»¶ç‰‡æ®µï¼Œä¸¦ç”Ÿæˆ 3 å€‹ã€Œä½¿ç”¨è€…æœ€å¯èƒ½æœƒå•çš„å•é¡Œã€ã€‚
        é€™äº›å•é¡Œå¿…é ˆèƒ½ç”±è©²ç‰‡æ®µå›ç­”ã€‚

        æ–‡ä»¶ç‰‡æ®µï¼š
        ---
        {node.text[:1500]} 
        ---

        å›æ‡‰æ ¼å¼è¦æ±‚ï¼š
        1. åªå›å‚³å•é¡Œï¼Œä¸€è¡Œä¸€å€‹ã€‚
        2. ä¸è¦åŠ ç·¨è™Ÿ (1. 2. 3.) æˆ–å…¶ä»–å»¢è©±ã€‚
        3. ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
        """

        try:
            response = await client.chat.completions.create(
                model="gpt-4o", # æˆ– gpt-3.5-turbo
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„å•é¡Œç”ŸæˆåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            content = response.choices[0].message.content.strip()
            # è™•ç†å›å‚³æ–‡å­—ï¼Œè®Šæˆ List
            questions = [line.strip() for line in content.split('\n') if line.strip()]
            
            # å­˜å…¥ metadata
            node.metadata["questions"] = questions
            # node.metadata["questions_text"] = "\n".join(questions) # å‚™ç”¨å­—ä¸²æ¬„ä½
            
            logger.info(f"âœ… Chunk {index+1} ç”Ÿæˆäº† {len(questions)} å€‹å•é¡Œ")

        except Exception as e:
            logger.error(f"âŒ Chunk {index+1} ç”Ÿæˆå¤±æ•—: {e}")
            node.metadata["questions"] = []

    # ç‚ºäº†é¿å…æ‰“çˆ† OpenAI Rate Limitï¼Œæˆ‘å€‘ç”¨ Semaphore é™åˆ¶ä½µç™¼æ•¸ (ä¾‹å¦‚ä¸€æ¬¡ 5 å€‹)
    sem = asyncio.Semaphore(5)

    async def sem_task(node, index):
        async with sem:
            await process_node(node, index)

    # å»ºç«‹æ‰€æœ‰ä»»å‹™ä¸¦åŸ·è¡Œ
    tasks = [sem_task(node, i) for i, node in enumerate(nodes)]
    await asyncio.gather(*tasks)

    logger.info("ğŸ‰ æ‰€æœ‰ç¯€é»çš„ NQ1D èƒå–å®Œæˆï¼")
    return nodes