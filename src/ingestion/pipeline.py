import sys
import logging
import time
from pathlib import Path

# è·¯å¾‘ä¿®æ­£ (é˜²æ­¢ ModuleNotFoundError)
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

from src.ingestion.parser import load_and_chunk_documents
from src.ingestion.extractor import SemanticExtractor
from src.ingestion.indexer import VectorIndexer

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),  # è¼¸å‡ºåˆ°è¢å¹•
        logging.FileHandler("ingestion.log", encoding='utf-8') # è¼¸å‡ºåˆ°æª”æ¡ˆ (æ–¹ä¾¿é™¤éŒ¯)
    ]
)
logger = logging.getLogger(__name__)

def run_pipeline(limit: int = 5):
    """
    åŸ·è¡Œå®Œæ•´çš„ RAG è³‡æ–™è™•ç†ç®¡ç·š (Blue Line)
    :param limit: ç‚ºäº†ç¯€çœ Tokenï¼Œé è¨­åªè™•ç†å‰ N å€‹ Chunksã€‚è¨­ç‚º None å‰‡è™•ç†å…¨éƒ¨ã€‚
    """
    logger.info("ğŸš€ å•Ÿå‹• RAG è³‡æ–™è™•ç†ç®¡ç·š (Phase 1 Full Pipeline)")
    
    # 1. è¼‰å…¥èˆ‡åˆ‡åˆ† (Parser)
    logger.info("Step 1: æ­£åœ¨è¼‰å…¥æ–‡ä»¶...")
    raw_nodes = load_and_chunk_documents()
    
    if not raw_nodes:
        logger.error("âŒ æ²’æœ‰è®€åˆ°ä»»ä½•æ–‡ä»¶ï¼Œæµç¨‹çµ‚æ­¢ã€‚")
        return

    logger.info(f"ğŸ“Š åŸå§‹æ–‡ä»¶å…±åˆ‡åˆ†ç‚º {len(raw_nodes)} å€‹ Chunksã€‚")
    
    # æ‡‰ç”¨ Limit é™åˆ¶
    target_nodes = raw_nodes[:limit] if limit else raw_nodes
    logger.info(f"âš ï¸ æ¸¬è©¦æ¨¡å¼ï¼šåƒ…è™•ç†å‰ {len(target_nodes)} å€‹ Chunks (Total: {len(raw_nodes)})")

    # 2. èªæ„èƒå– (Extractor)
    logger.info("Step 2: é–‹å§‹ AI èªæ„èƒå– (é€™éœ€è¦ä¸€é»æ™‚é–“)...")
    extractor = SemanticExtractor()
    processed_chunks = []

    start_time = time.time()
    for i, node in enumerate(target_nodes):
        logger.info(f"ğŸ¤– Processing Chunk {i+1}/{len(target_nodes)} ...")
        result = extractor.extract(node)
        
        if result:
            processed_chunks.append(result)
        else:
            logger.warning(f"âš ï¸ Chunk {i+1} èƒå–å¤±æ•—ï¼Œè·³éã€‚")

    duration = time.time() - start_time
    logger.info(f"âœ… èƒå–å®Œæˆï¼è€—æ™‚ {duration:.2f} ç§’ã€‚æˆåŠŸç‡: {len(processed_chunks)}/{len(target_nodes)}")

    # 3. å‘é‡å…¥åº« (Indexer)
    if processed_chunks:
        logger.info("Step 3: å¯«å…¥å‘é‡è³‡æ–™åº« (Qdrant)...")
        indexer = VectorIndexer()
        indexer.index(processed_chunks)
        logger.info("ğŸ‰ Pipeline åŸ·è¡Œå®Œç•¢ï¼è³‡æ–™å·²å…¥åº«ã€‚")
    else:
        logger.warning("âŒ æ²’æœ‰æœ‰æ•ˆçš„è³‡æ–™å¯ä»¥å…¥åº«ã€‚")

if __name__ == "__main__":
    # åŸ·è¡Œç®¡ç·š (é è¨­åªè·‘ 5 ç­†)
    # æƒ³è·‘å…¨éƒ¨è«‹æ”¹ç”¨: run_pipeline(limit=None)
    run_pipeline(limit=5)