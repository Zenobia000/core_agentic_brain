import sys
import logging
import asyncio
from pathlib import Path

# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

# å¼•å…¥æˆ‘å€‘ä¹‹å‰å¯«å¥½çš„æ¨¡çµ„
from src.ingestion.parser import load_and_chunk_documents
from src.ingestion.extractor import extract_nq1d
from src.ingestion.indexer import index_nodes

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

async def run_pipeline(target_filename: str = None):
    """
    åŸ·è¡Œ RAG è³‡æ–™è™•ç†ç®¡ç·š
    :param target_filename: å¦‚æœæœ‰æŒ‡å®šï¼Œåªè™•ç†é€™å€‹æª”æ¡ˆ (å°šæœªå¯¦ä½œå–®æª”éæ¿¾ï¼Œç›®å‰ä»æƒæç›®éŒ„ï¼Œä½†å¯ç”¨æ–¼æ“´å……)
    """
    logger.info("ğŸš€ å•Ÿå‹• RAG è³‡æ–™è™•ç†ç®¡ç·š (Phase 1 Full Pipeline)")

    # Step 1: Parsing (Docling + Cohere)
    # ç›®å‰ parser é è¨­æœƒæƒæ data/raw ä¸‹çš„æ‰€æœ‰æª”æ¡ˆ
    # è‹¥è¦å„ªåŒ–æ•ˆèƒ½ï¼Œæœªä¾†å¯ä»¥è®“ parser æ”¯æ´åªè®€ç‰¹å®šæª”æ¡ˆ
    logger.info("Step 1: æ­£åœ¨è¼‰å…¥èˆ‡åˆ‡åˆ†æ–‡ä»¶...")
    nodes = load_and_chunk_documents(data_dir="data/raw")
    
    if not nodes:
        logger.warning("âš ï¸ æ²’æœ‰ç”¢ç”Ÿä»»ä½•ç¯€é»ï¼ŒçµæŸç®¡ç·šã€‚")
        return {"status": "empty", "processed_docs": 0}

    logger.info(f"ğŸ“Š åŸå§‹æ–‡ä»¶å…±åˆ‡åˆ†ç‚º {len(nodes)} å€‹ Chunksã€‚")

    # Step 2: Extracting (NQ1D)
    logger.info("Step 2: é–‹å§‹ AI èªæ„èƒå– (é€™éœ€è¦ä¸€é»æ™‚é–“)...")
    # é€™è£¡å¯ä»¥è€ƒæ…®åªå°æ–°é€²æª”æ¡ˆåšèƒå–ï¼Œç›®å‰å…ˆå…¨é‡è™•ç†
    nodes = await extract_nq1d(nodes)

    # Step 3: Indexing (Qdrant)
    logger.info("Step 3: å¯«å…¥å‘é‡è³‡æ–™åº« (Qdrant)...")
    success_count = await index_nodes(nodes)
    
    logger.info("ğŸ‰ Pipeline åŸ·è¡Œå®Œç•¢ï¼è³‡æ–™å·²å…¥åº«ã€‚")
    return {"status": "success", "processed_chunks": len(nodes), "indexed_count": success_count}

if __name__ == "__main__":
    # å¦‚æœæ˜¯ç›´æ¥åŸ·è¡Œè…³æœ¬
    asyncio.run(run_pipeline())