"""
RAG MCP Server - FastMCP ç‰ˆæœ¬ v3
æä¾› RAG çŸ¥è­˜åº«å·¥å…·çµ¦ OpenCode ä½¿ç”¨

æ›´æ–°å…§å®¹ v3:
- æ–°å¢è¯ç¶²æœå°‹åŠŸèƒ½ (DuckDuckGo)
- æ–°å¢ rag_ask_with_web æ•´åˆå•ç­”
- æ”¹é€²æ‰€æœ‰ Tool çš„æè¿° (æç¤ºè©å·¥ç¨‹)
- ä¿®æ­£ rag_list_documents æ ¼å¼è™•ç†
- çµæœç·©å­˜åŠŸèƒ½

å•Ÿå‹•æ–¹å¼:
    python -m src.mcp.server

æ¸¬è©¦æ–¹å¼:
    mcp dev src/mcp/server.py
"""

import os
import sys
import asyncio
import hashlib
import time
from pathlib import Path
from typing import Optional, Dict, Any, List
from collections import OrderedDict

# ç¢ºä¿å¯ä»¥ import å°ˆæ¡ˆæ¨¡çµ„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import httpx
from mcp.server.fastmcp import FastMCP

# å˜—è©¦å°å…¥ DuckDuckGo æœå°‹
try:
    from duckduckgo_search import DDGS
    SEARCH_AVAILABLE = True
except ImportError:
    SEARCH_AVAILABLE = False
    print("âš ï¸ duckduckgo_search æœªå®‰è£ï¼Œè¯ç¶²æœå°‹åŠŸèƒ½å°‡ä¸å¯ç”¨")
    print("   å®‰è£æŒ‡ä»¤: pip install duckduckgo_search")

# å»ºç«‹ MCP Server
mcp = FastMCP("rag-server")

# RAG API è¨­å®š
RAG_API_BASE = os.getenv("RAG_API_BASE", "http://localhost:8001")
TIMEOUT = 120.0  # ä¸Šå‚³å¤§æª”æ¡ˆéœ€è¦è¼ƒé•·æ™‚é–“

# ============================================================
# ç·©å­˜ç³»çµ±
# ============================================================

class LRUCache:
    """ç°¡å–®çš„ LRU ç·©å­˜ï¼Œæ”¯æ´éæœŸæ™‚é–“"""
    
    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):
        self.cache: OrderedDict = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl_seconds
    
    def _make_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç·©å­˜ key"""
        key_str = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """å–å¾—ç·©å­˜å€¼ï¼Œå¦‚æœéæœŸå‰‡è¿”å› None"""
        if key not in self.cache:
            return None
        
        value, timestamp = self.cache[key]
        if time.time() - timestamp > self.ttl:
            del self.cache[key]
            return None
        
        # ç§»åˆ°æœ€å¾Œ (LRU)
        self.cache.move_to_end(key)
        return value
    
    def set(self, key: str, value: Any):
        """è¨­å®šç·©å­˜å€¼"""
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = (value, time.time())
        
        # è¶…éå¤§å°é™åˆ¶æ™‚ç§»é™¤æœ€èˆŠçš„
        while len(self.cache) > self.max_size:
            self.cache.popitem(last=False)
    
    def clear(self):
        """æ¸…ç©ºç·©å­˜"""
        self.cache.clear()
    
    def stats(self) -> Dict[str, Any]:
        """ç·©å­˜çµ±è¨ˆ"""
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "ttl_seconds": self.ttl
        }

# å»ºç«‹ç·©å­˜å¯¦ä¾‹
search_cache = LRUCache(max_size=100, ttl_seconds=1800)  # æœå°‹ç·©å­˜ 30 åˆ†é˜
ask_cache = LRUCache(max_size=50, ttl_seconds=3600)      # å•ç­”ç·©å­˜ 1 å°æ™‚
web_cache = LRUCache(max_size=50, ttl_seconds=600)       # ç¶²è·¯æœå°‹ç·©å­˜ 10 åˆ†é˜

# ============================================================
# MCP Tools - æ ¸å¿ƒ RAG åŠŸèƒ½
# ============================================================

@mcp.tool()
async def rag_search(query: str, top_k: int = 5, use_cache: bool = True) -> str:
    """
    åœ¨ä¼æ¥­çŸ¥è­˜åº«ä¸­é€²è¡Œèªæ„æœå°‹ï¼Œæ‰¾å‡ºèˆ‡å•é¡Œæœ€ç›¸é—œçš„æ–‡ä»¶æ®µè½ã€‚
    
    é€™å€‹å·¥å…·é©åˆç”¨æ–¼ï¼š
    - æŸ¥æ‰¾æŠ€è¡“æ–‡ä»¶ã€è«–æ–‡ã€ç”¢å“èªªæ˜ä¸­çš„ç‰¹å®šå…§å®¹
    - äº†è§£æŸå€‹æ¦‚å¿µã€æŠ€è¡“æˆ–è¡“èªçš„å®šç¾©
    - æ”¶é›†å¤šå€‹ç›¸é—œæ®µè½ä»¥ä¾¿é€²è¡Œæ¯”è¼ƒåˆ†æ
    - åœ¨å›ç­”å•é¡Œå‰å…ˆæ”¶é›†èƒŒæ™¯è³‡æ–™
    
    ä½¿ç”¨å»ºè­°ï¼š
    - ä½¿ç”¨å®Œæ•´çš„å¥å­æˆ–å•é¡Œä½œç‚º queryï¼Œæ•ˆæœæ¯”é—œéµå­—æ›´å¥½
    - å¦‚æœçµæœä¸å¤ ç²¾ç¢ºï¼Œå˜—è©¦æ›å€‹æ–¹å¼æè¿°å•é¡Œ
    - top_k å»ºè­°è¨­ 3-10ï¼Œå¤ªå¤šæœƒåŒ…å«ä¸ç›¸é—œå…§å®¹
    
    Args:
        query: æœå°‹å•é¡Œæˆ–é—œéµå­—ï¼ˆå»ºè­°ä½¿ç”¨å®Œæ•´å¥å­ï¼‰
        top_k: è¿”å›çµæœæ•¸é‡ï¼Œé è¨­ 5ï¼Œç¯„åœ 1-20
        use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜ï¼Œé è¨­ True
    
    Returns:
        æœå°‹çµæœåˆ—è¡¨ï¼Œæ¯å€‹çµæœåŒ…å«ï¼š
        - ä¾†æºæª”æ¡ˆåç¨±å’Œé ç¢¼
        - ç›¸é—œåº¦åˆ†æ•¸ (0-1ï¼Œè¶Šé«˜è¶Šç›¸é—œ)
        - æ–‡ä»¶å…§å®¹ç‰‡æ®µ
    """
    # æª¢æŸ¥ç·©å­˜
    cache_key = search_cache._make_key(query, top_k)
    if use_cache:
        cached = search_cache.get(cache_key)
        if cached:
            return f"[ç·©å­˜çµæœ]\n{cached}"
    
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            response = await client.post(
                f"{RAG_API_BASE}/search",
                json={"query": query, "top_k": top_k}
            )
            response.raise_for_status()
            results = response.json()
            
            if not results:
                return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œçµæœã€‚å»ºè­°ï¼š\n1. å˜—è©¦ä½¿ç”¨ä¸åŒçš„é—œéµå­—\n2. ç¢ºèªçŸ¥è­˜åº«ä¸­æ˜¯å¦æœ‰ç›¸é—œæ–‡ä»¶"
            
            # æ ¼å¼åŒ–è¼¸å‡º
            output = [f"ğŸ” æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœï¼š\n"]
            for i, r in enumerate(results, 1):
                source = r.get('source', 'unknown')
                page = r.get('page', '?')
                score = r.get('score', 0)
                text = r.get('text', '')[:300]
                
                output.append(f"ã€{i}ã€‘{source} (ç¬¬ {page} é )")
                output.append(f"    ç›¸é—œåº¦: {score:.1%}")
                output.append(f"    å…§å®¹: {text}...")
                output.append("")
            
            result_text = "\n".join(output)
            
            # å­˜å…¥ç·©å­˜
            if use_cache:
                search_cache.set(cache_key, result_text)
            
            return result_text
            
        except httpx.HTTPError as e:
            return f"âŒ æœå°‹å¤±æ•—: {str(e)}\nè«‹ç¢ºèª RAG API æœå‹™æ˜¯å¦æ­£å¸¸é‹è¡Œ (http://localhost:8001)"


@mcp.tool()
async def rag_ask(question: str, top_k: int = 5, use_cache: bool = True) -> str:
    """
    å‘çŸ¥è­˜åº«æå•ä¸¦ç²å¾— AI ç”Ÿæˆçš„å›ç­”ï¼Œå›ç­”æœƒåŸºæ–¼çŸ¥è­˜åº«ä¸­çš„å¯¦éš›å…§å®¹ã€‚
    
    é€™å€‹å·¥å…·é©åˆç”¨æ–¼ï¼š
    - éœ€è¦ç¶œåˆå¤šå€‹æ–‡ä»¶å…§å®¹ä¾†å›ç­”çš„å•é¡Œ
    - å¸Œæœ›å¾—åˆ°æœ‰å¼•ç”¨ä¾†æºçš„ç­”æ¡ˆ
    - è©¢å•çŸ¥è­˜åº«ä¸­æ–‡ä»¶çš„å…·é«”å…§å®¹
    - æ¯”è¼ƒã€ç¸½çµã€åˆ†æçŸ¥è­˜åº«ä¸­çš„è³‡è¨Š
    
    èˆ‡ rag_search çš„å€åˆ¥ï¼š
    - rag_search: åªè¿”å›åŸå§‹æ–‡ä»¶ç‰‡æ®µï¼Œéœ€è¦è‡ªå·±é–±è®€ç†è§£
    - rag_ask: è¿”å› AI æ•´ç†éçš„ç­”æ¡ˆï¼Œä¸¦æ¨™è¨»å¼•ç”¨ä¾†æº
    
    ä½¿ç”¨å»ºè­°ï¼š
    - å•é¡Œè¦æ˜ç¢ºå…·é«”ï¼Œé¿å…å¤ªç± çµ±
    - å¦‚æœç­”æ¡ˆä¸æ»¿æ„ï¼Œå¯ä»¥è¿½å•æˆ–æ›å€‹è§’åº¦æå•
    
    Args:
        question: è¦å•çš„å•é¡Œï¼ˆå»ºè­°ä½¿ç”¨å®Œæ•´çš„å•å¥ï¼‰
        top_k: åƒè€ƒçš„æ–‡ä»¶æ•¸é‡ï¼Œé è¨­ 5
        use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜ï¼Œé è¨­ True
    
    Returns:
        AI ç”Ÿæˆçš„å›ç­”ï¼ŒåŒ…å«ï¼š
        - åŸºæ–¼çŸ¥è­˜åº«å…§å®¹çš„ç­”æ¡ˆ
        - åƒè€ƒä¾†æºåˆ—è¡¨ï¼ˆæª”æ¡ˆåç¨±å’Œé ç¢¼ï¼‰
    """
    # æª¢æŸ¥ç·©å­˜
    cache_key = ask_cache._make_key(question, top_k)
    if use_cache:
        cached = ask_cache.get(cache_key)
        if cached:
            return f"[ç·©å­˜çµæœ]\n{cached}"
    
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            response = await client.post(
                f"{RAG_API_BASE}/ask",
                json={"question": question, "top_k": top_k}
            )
            response.raise_for_status()
            result = response.json()
            
            answer = result.get("answer", "ç„¡æ³•ç”Ÿæˆå›ç­”")
            sources = result.get("sources", [])
            
            output = [answer, "", "ğŸ“š åƒè€ƒä¾†æº:"]
            for s in sources:
                source = s.get('source', 'unknown')
                page = s.get('page', '?')
                output.append(f"  â€¢ {source} (ç¬¬ {page} é )")
            
            result_text = "\n".join(output)
            
            # å­˜å…¥ç·©å­˜
            if use_cache:
                ask_cache.set(cache_key, result_text)
            
            return result_text
            
        except httpx.HTTPError as e:
            return f"âŒ æå•å¤±æ•—: {str(e)}\nè«‹ç¢ºèª RAG API æœå‹™æ˜¯å¦æ­£å¸¸é‹è¡Œ"


# ============================================================
# MCP Tools - è¯ç¶²æœå°‹åŠŸèƒ½ (æ–°å¢)
# ============================================================

@mcp.tool()
async def web_search(query: str, max_results: int = 5, use_cache: bool = True) -> str:
    """
    ä½¿ç”¨ DuckDuckGo é€²è¡Œç¶²è·¯æœå°‹ï¼Œç²å–æœ€æ–°çš„ç¶²è·¯è³‡è¨Šã€‚
    
    é€™å€‹å·¥å…·é©åˆç”¨æ–¼ï¼š
    - æŸ¥æ‰¾çŸ¥è­˜åº«ä¸­æ²’æœ‰çš„æœ€æ–°è³‡è¨Š
    - è£œå……çŸ¥è­˜åº«å…§å®¹çš„ä¸è¶³
    - é©—è­‰æˆ–æ›´æ–°éæ™‚çš„è³‡è¨Š
    - æœå°‹æ–°èã€è¶¨å‹¢ã€æœ€æ–°ç™¼å±•
    
    èˆ‡ rag_search çš„å€åˆ¥ï¼š
    - rag_search: æœå°‹æœ¬åœ°çŸ¥è­˜åº«ï¼ˆPDF æ–‡ä»¶ï¼‰
    - web_search: æœå°‹ç¶²éš›ç¶²è·¯ï¼ˆæœ€æ–°è³‡è¨Šï¼‰
    
    Args:
        query: æœå°‹é—œéµå­—æˆ–å•é¡Œ
        max_results: è¿”å›çµæœæ•¸é‡ï¼Œé è¨­ 5ï¼Œæœ€å¤š 10
        use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜ï¼Œé è¨­ Trueï¼ˆç·©å­˜ 10 åˆ†é˜ï¼‰
    
    Returns:
        ç¶²è·¯æœå°‹çµæœï¼ŒåŒ…å«æ¨™é¡Œã€æ‘˜è¦å’Œé€£çµ
    """
    if not SEARCH_AVAILABLE:
        return "âŒ è¯ç¶²æœå°‹åŠŸèƒ½æœªå•Ÿç”¨\n\nè«‹å®‰è£ duckduckgo_search:\npip install duckduckgo_search"
    
    # é™åˆ¶çµæœæ•¸é‡
    max_results = min(max_results, 10)
    
    # æª¢æŸ¥ç·©å­˜
    cache_key = web_cache._make_key(query, max_results)
    if use_cache:
        cached = web_cache.get(cache_key)
        if cached:
            return f"[ç·©å­˜çµæœ - 10åˆ†é˜å…§]\n{cached}"
    
    try:
        # ä½¿ç”¨ DuckDuckGo æœå°‹
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=max_results))
        
        if not results:
            return f"ğŸ” æ²’æœ‰æ‰¾åˆ°èˆ‡ '{query}' ç›¸é—œçš„ç¶²è·¯çµæœ"
        
        # æ ¼å¼åŒ–è¼¸å‡º
        output = [f"ğŸŒ ç¶²è·¯æœå°‹çµæœ ({len(results)} ç­†)ï¼š\n"]
        
        for i, r in enumerate(results, 1):
            title = r.get('title', 'ç„¡æ¨™é¡Œ')
            body = r.get('body', '')[:200]
            href = r.get('href', '')
            
            output.append(f"ã€{i}ã€‘{title}")
            output.append(f"    {body}...")
            output.append(f"    ğŸ”— {href}")
            output.append("")
        
        result_text = "\n".join(output)
        
        # å­˜å…¥ç·©å­˜
        if use_cache:
            web_cache.set(cache_key, result_text)
        
        return result_text
        
    except Exception as e:
        return f"âŒ ç¶²è·¯æœå°‹å¤±æ•—: {str(e)}"


@mcp.tool()
async def web_search_news(query: str, max_results: int = 5) -> str:
    """
    æœå°‹æœ€æ–°æ–°èï¼Œç²å–ç‰¹å®šä¸»é¡Œçš„æ–°èå ±å°ã€‚
    
    é©åˆç”¨æ–¼ï¼š
    - äº†è§£æŸå€‹ä¸»é¡Œçš„æœ€æ–°ç™¼å±•
    - è¿½è¹¤ç”¢æ¥­å‹•æ…‹
    - ç²å–æ™‚äº‹è³‡è¨Š
    
    Args:
        query: æ–°èæœå°‹é—œéµå­—
        max_results: è¿”å›çµæœæ•¸é‡ï¼Œé è¨­ 5
    
    Returns:
        æ–°èæœå°‹çµæœï¼ŒåŒ…å«æ¨™é¡Œã€æ—¥æœŸã€ä¾†æºå’Œæ‘˜è¦
    """
    if not SEARCH_AVAILABLE:
        return "âŒ è¯ç¶²æœå°‹åŠŸèƒ½æœªå•Ÿç”¨\n\nè«‹å®‰è£ duckduckgo_search:\npip install duckduckgo_search"
    
    try:
        with DDGS() as ddgs:
            results = list(ddgs.news(query, max_results=max_results))
        
        if not results:
            return f"ğŸ“° æ²’æœ‰æ‰¾åˆ°èˆ‡ '{query}' ç›¸é—œçš„æ–°è"
        
        output = [f"ğŸ“° æ–°èæœå°‹çµæœ ({len(results)} ç­†)ï¼š\n"]
        
        for i, r in enumerate(results, 1):
            title = r.get('title', 'ç„¡æ¨™é¡Œ')
            body = r.get('body', '')[:150]
            source = r.get('source', 'æœªçŸ¥ä¾†æº')
            date = r.get('date', '')
            url = r.get('url', '')
            
            output.append(f"ã€{i}ã€‘{title}")
            output.append(f"    ğŸ“… {date} | ğŸ“° {source}")
            output.append(f"    {body}...")
            output.append(f"    ğŸ”— {url}")
            output.append("")
        
        return "\n".join(output)
        
    except Exception as e:
        return f"âŒ æ–°èæœå°‹å¤±æ•—: {str(e)}"


@mcp.tool()
async def rag_ask_with_web(
    question: str, 
    rag_top_k: int = 3, 
    web_results: int = 3
) -> str:
    """
    çµåˆçŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹çš„æ™ºæ…§å•ç­”ï¼Œæä¾›æ›´å®Œæ•´çš„ç­”æ¡ˆã€‚
    
    é€™å€‹å·¥å…·æœƒï¼š
    1. å…ˆæœå°‹æœ¬åœ°çŸ¥è­˜åº«ï¼ˆå·²ä¸Šå‚³çš„ PDFï¼‰
    2. å†æœå°‹ç¶²è·¯ç²å–è£œå……è³‡è¨Š
    3. æ•´åˆå…©è€…ç”Ÿæˆæ›´å…¨é¢çš„å›ç­”
    
    é©åˆç”¨æ–¼ï¼š
    - çŸ¥è­˜åº«è³‡è¨Šå¯èƒ½éæ™‚ï¼Œéœ€è¦æœ€æ–°è³‡è¨Šè£œå……
    - å•é¡Œæ¶‰åŠçŸ¥è­˜åº«ä»¥å¤–çš„å…§å®¹
    - éœ€è¦æ¯”è¼ƒå…§éƒ¨æ–‡ä»¶å’Œå¤–éƒ¨è³‡è¨Š
    
    èˆ‡å…¶ä»–å·¥å…·çš„å€åˆ¥ï¼š
    - rag_ask: åªç”¨çŸ¥è­˜åº«å›ç­”
    - web_search: åªæœå°‹ç¶²è·¯
    - rag_ask_with_web: æ•´åˆå…©è€…ï¼Œæ›´å®Œæ•´
    
    Args:
        question: è¦å•çš„å•é¡Œ
        rag_top_k: çŸ¥è­˜åº«æœå°‹çµæœæ•¸é‡ï¼Œé è¨­ 3
        web_results: ç¶²è·¯æœå°‹çµæœæ•¸é‡ï¼Œé è¨­ 3
    
    Returns:
        æ•´åˆçŸ¥è­˜åº«å’Œç¶²è·¯è³‡è¨Šçš„å®Œæ•´å›ç­”
    """
    results = []
    
    # 1. æœå°‹çŸ¥è­˜åº«
    results.append("ğŸ“š **çŸ¥è­˜åº«æœå°‹çµæœï¼š**\n")
    try:
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            response = await client.post(
                f"{RAG_API_BASE}/search",
                json={"query": question, "top_k": rag_top_k}
            )
            response.raise_for_status()
            rag_results = response.json()
            
            if rag_results:
                for i, r in enumerate(rag_results, 1):
                    source = r.get('source', 'unknown')
                    page = r.get('page', '?')
                    text = r.get('text', '')[:200]
                    results.append(f"[{i}] {source} (p.{page}): {text}...")
            else:
                results.append("ï¼ˆçŸ¥è­˜åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹ï¼‰")
    except Exception as e:
        results.append(f"ï¼ˆçŸ¥è­˜åº«æœå°‹å¤±æ•—: {e}ï¼‰")
    
    results.append("\n")
    
    # 2. æœå°‹ç¶²è·¯
    results.append("ğŸŒ **ç¶²è·¯æœå°‹çµæœï¼š**\n")
    if SEARCH_AVAILABLE:
        try:
            with DDGS() as ddgs:
                web_results_data = list(ddgs.text(question, max_results=web_results))
            
            if web_results_data:
                for i, r in enumerate(web_results_data, 1):
                    title = r.get('title', '')
                    body = r.get('body', '')[:150]
                    results.append(f"[{i}] {title}: {body}...")
            else:
                results.append("ï¼ˆç¶²è·¯ä¸Šæ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹ï¼‰")
        except Exception as e:
            results.append(f"ï¼ˆç¶²è·¯æœå°‹å¤±æ•—: {e}ï¼‰")
    else:
        results.append("ï¼ˆè¯ç¶²æœå°‹åŠŸèƒ½æœªå•Ÿç”¨ï¼‰")
    
    results.append("\n")
    
    # 3. ä½¿ç”¨ RAG API ç”Ÿæˆæ•´åˆå›ç­”
    results.append("ğŸ’¡ **æ•´åˆå›ç­”ï¼š**\n")
    try:
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            response = await client.post(
                f"{RAG_API_BASE}/ask",
                json={"question": question, "top_k": rag_top_k}
            )
            response.raise_for_status()
            answer_data = response.json()
            answer = answer_data.get("answer", "ç„¡æ³•ç”Ÿæˆå›ç­”")
            results.append(answer)
    except Exception as e:
        results.append(f"ï¼ˆå›ç­”ç”Ÿæˆå¤±æ•—: {e}ï¼‰")
    
    return "\n".join(results)


# ============================================================
# MCP Tools - æ–‡ä»¶ç®¡ç†
# ============================================================

@mcp.tool()
async def rag_list_documents() -> str:
    """
    åˆ—å‡ºçŸ¥è­˜åº«ä¸­æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ä»¶æ¸…å–®ã€‚
    
    é€™å€‹å·¥å…·é©åˆç”¨æ–¼ï¼š
    - äº†è§£ç›®å‰çŸ¥è­˜åº«åŒ…å«å“ªäº›æ–‡ä»¶
    - ç¢ºèªæŸå€‹æ–‡ä»¶æ˜¯å¦å·²ç¶“ä¸Šå‚³ä¸¦ç´¢å¼•
    - åœ¨æœå°‹å‰å…ˆäº†è§£çŸ¥è­˜åº«çš„ç¯„åœ
    
    Returns:
        å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«æª”æ¡ˆåç¨±
    """
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            response = await client.get(f"{RAG_API_BASE}/documents")
            response.raise_for_status()
            docs = response.json()
            
            if not docs:
                return "ğŸ“­ çŸ¥è­˜åº«ç›®å‰æ²’æœ‰ä»»ä½•æ–‡ä»¶ã€‚\n\nä½¿ç”¨ rag_upload ä¸Šå‚³ PDF æ–‡ä»¶ä¾†å»ºç«‹çŸ¥è­˜åº«ã€‚"
            
            output = [f"ğŸ“š çŸ¥è­˜åº«æ–‡ä»¶æ¸…å–® (å…± {len(docs)} å€‹)ï¼š", ""]
            
            for i, doc in enumerate(docs, 1):
                # è™•ç†å­—ä¸²åˆ—è¡¨æˆ–ç‰©ä»¶åˆ—è¡¨
                if isinstance(doc, str):
                    output.append(f"  {i}. ğŸ“„ {doc}")
                elif isinstance(doc, dict):
                    name = doc.get("name", doc.get("filename", "unknown"))
                    chunks = doc.get("chunks", "?")
                    status = doc.get("status", "")
                    output.append(f"  {i}. ğŸ“„ {name}")
                    if chunks != "?":
                        output.append(f"      å€å¡Šæ•¸: {chunks}")
                    if status:
                        output.append(f"      ç‹€æ…‹: {status}")
                else:
                    output.append(f"  {i}. ğŸ“„ {str(doc)}")
            
            return "\n".join(output)
            
        except httpx.HTTPError as e:
            return f"âŒ å–å¾—æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {str(e)}"


@mcp.tool()
async def rag_get_stats() -> str:
    """
    å–å¾—çŸ¥è­˜åº«çš„çµ±è¨ˆè³‡è¨Šï¼ŒåŒ…æ‹¬æ–‡ä»¶æ•¸é‡ã€å‘é‡æ•¸é‡ç­‰ã€‚
    
    é€™å€‹å·¥å…·é©åˆç”¨æ–¼ï¼š
    - äº†è§£çŸ¥è­˜åº«çš„æ•´é«”è¦æ¨¡
    - ç›£æ§çŸ¥è­˜åº«çš„ä½¿ç”¨ç‹€æ³
    - æ’æŸ¥å•é¡Œæ™‚ç¢ºèªç³»çµ±ç‹€æ…‹
    
    Returns:
        çŸ¥è­˜åº«çµ±è¨ˆæ•¸æ“šï¼ŒåŒ…å«ï¼š
        - æ–‡ä»¶æ•¸é‡
        - ç¸½å€å¡Šæ•¸ï¼ˆå‘é‡æ•¸é‡ï¼‰
        - å‘é‡ç¶­åº¦
        - ç´¢å¼•å¤§å°
    """
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            response = await client.get(f"{RAG_API_BASE}/stats")
            response.raise_for_status()
            stats = response.json()
            
            output = [
                "ğŸ“Š çŸ¥è­˜åº«çµ±è¨ˆè³‡è¨Š",
                "â•" * 30,
                f"ğŸ“ æ–‡ä»¶æ•¸é‡: {stats.get('document_count', 0)} å€‹",
                f"ğŸ§© ç¸½å€å¡Šæ•¸: {stats.get('total_chunks', 0)} å€‹",
                f"ğŸ“ å‘é‡ç¶­åº¦: {stats.get('vector_dim', 'N/A')}",
                f"ğŸ’¾ ç´¢å¼•å¤§å°: {stats.get('index_size', 'N/A')}",
                "",
                "ğŸ“¦ ç·©å­˜ç‹€æ…‹:",
                f"   æœå°‹ç·©å­˜: {search_cache.stats()['size']}/{search_cache.stats()['max_size']}",
                f"   å•ç­”ç·©å­˜: {ask_cache.stats()['size']}/{ask_cache.stats()['max_size']}",
                f"   ç¶²è·¯ç·©å­˜: {web_cache.stats()['size']}/{web_cache.stats()['max_size']}",
                "",
                f"ğŸŒ è¯ç¶²æœå°‹: {'âœ… å·²å•Ÿç”¨' if SEARCH_AVAILABLE else 'âŒ æœªå®‰è£'}",
            ]
            
            return "\n".join(output)
            
        except httpx.HTTPError as e:
            return f"âŒ å–å¾—çµ±è¨ˆè³‡è¨Šå¤±æ•—: {str(e)}"


@mcp.tool()
async def rag_upload(file_path: str) -> str:
    """
    ä¸Šå‚³å–®ä¸€ PDF æ–‡ä»¶åˆ°çŸ¥è­˜åº«é€²è¡Œç´¢å¼•ã€‚
    
    ä¸Šå‚³å¾Œç³»çµ±æœƒï¼š
    1. è§£æ PDF å…§å®¹ï¼ˆä½¿ç”¨ IBM Doclingï¼‰
    2. å°‡å…§å®¹åˆ‡åˆ†æˆå°æ®µè½
    3. ç”Ÿæˆå‘é‡ä¸¦å­˜å…¥ Qdrant
    
    æ³¨æ„äº‹é …ï¼š
    - åªæ”¯æ´ PDF æ ¼å¼
    - å¤§æª”æ¡ˆï¼ˆ>10MBï¼‰å»ºè­°å…ˆåˆ†å‰²
    - ä¸Šå‚³éœ€è¦ä¸€äº›æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…
    
    Args:
        file_path: PDF æª”æ¡ˆçš„å®Œæ•´è·¯å¾‘
    
    Returns:
        ä¸Šå‚³çµæœç‹€æ…‹
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        return f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}\nè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢º"
    
    if not file_path.suffix.lower() == ".pdf":
        return f"âŒ åªæ”¯æ´ PDF æª”æ¡ˆï¼Œç›®å‰æª”æ¡ˆæ ¼å¼: {file_path.suffix}"
    
    file_size = file_path.stat().st_size / (1024 * 1024)  # MB
    if file_size > 10:
        return f"âš ï¸ æª”æ¡ˆè¼ƒå¤§ ({file_size:.1f} MB)ï¼Œä¸Šå‚³å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“..."
    
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            with open(file_path, "rb") as f:
                files = {"file": (file_path.name, f, "application/pdf")}
                response = await client.post(
                    f"{RAG_API_BASE}/upload",
                    files=files
                )
            response.raise_for_status()
            result = response.json()
            return f"âœ… ä¸Šå‚³æˆåŠŸï¼\nğŸ“„ æª”æ¡ˆ: {file_path.name}\nğŸ’¬ {result.get('message', 'å·²åŠ å…¥ç´¢å¼•')}"
            
        except httpx.HTTPError as e:
            return f"âŒ ä¸Šå‚³å¤±æ•—: {str(e)}"


@mcp.tool()
async def rag_upload_batch(file_paths: list[str], delay_seconds: float = 2.0) -> str:
    """
    æ‰¹æ¬¡ä¸Šå‚³å¤šå€‹ PDF æ–‡ä»¶åˆ°çŸ¥è­˜åº«ã€‚
    
    é©åˆç”¨æ–¼ï¼š
    - ä¸€æ¬¡ä¸Šå‚³å¤šå€‹ç›¸é—œæ–‡ä»¶
    - åˆå§‹åŒ–çŸ¥è­˜åº«æ™‚æ‰¹é‡å°å…¥
    
    Args:
        file_paths: PDF æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        delay_seconds: æ¯å€‹æª”æ¡ˆä¸Šå‚³é–“éš”ç§’æ•¸ï¼Œé è¨­ 2 ç§’ï¼ˆé¿å…éè¼‰ï¼‰
    
    Returns:
        æ‰¹æ¬¡ä¸Šå‚³çµæœæ‘˜è¦
    """
    results = []
    success_count = 0
    fail_count = 0
    
    for i, file_path in enumerate(file_paths, 1):
        path = Path(file_path)
        
        if not path.exists():
            results.append(f"âŒ [{i}/{len(file_paths)}] {path.name}: æª”æ¡ˆä¸å­˜åœ¨")
            fail_count += 1
            continue
        
        if not path.suffix.lower() == ".pdf":
            results.append(f"âŒ [{i}/{len(file_paths)}] {path.name}: ä¸æ˜¯ PDF æª”æ¡ˆ")
            fail_count += 1
            continue
        
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            try:
                with open(path, "rb") as f:
                    files = {"file": (path.name, f, "application/pdf")}
                    response = await client.post(
                        f"{RAG_API_BASE}/upload",
                        files=files
                    )
                response.raise_for_status()
                results.append(f"âœ… [{i}/{len(file_paths)}] {path.name}: ä¸Šå‚³æˆåŠŸ")
                success_count += 1
                
            except httpx.HTTPError as e:
                results.append(f"âŒ [{i}/{len(file_paths)}] {path.name}: {str(e)}")
                fail_count += 1
        
        # é–“éš”ç­‰å¾…ï¼Œé¿å…éè¼‰
        if i < len(file_paths):
            await asyncio.sleep(delay_seconds)
    
    # ç¸½çµ
    summary = [
        "â•" * 40,
        f"ğŸ“Š æ‰¹æ¬¡ä¸Šå‚³å®Œæˆ",
        f"   âœ… æˆåŠŸ: {success_count} å€‹",
        f"   âŒ å¤±æ•—: {fail_count} å€‹",
        "â•" * 40,
        ""
    ]
    
    return "\n".join(summary + results)


@mcp.tool()
async def rag_delete_document(document_name: str) -> str:
    """
    å¾çŸ¥è­˜åº«åˆªé™¤æŒ‡å®šæ–‡ä»¶ã€‚
    
    æ³¨æ„ï¼šåˆªé™¤å¾Œç„¡æ³•æ¢å¾©ï¼Œè«‹è¬¹æ…æ“ä½œã€‚
    
    Args:
        document_name: è¦åˆªé™¤çš„æ–‡ä»¶åç¨±ï¼ˆå¯ç”¨ rag_list_documents æŸ¥çœ‹ï¼‰
    
    Returns:
        åˆªé™¤çµæœ
    """
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            response = await client.delete(
                f"{RAG_API_BASE}/documents/{document_name}"
            )
            response.raise_for_status()
            
            # æ¸…ç©ºç›¸é—œç·©å­˜
            search_cache.clear()
            ask_cache.clear()
            
            return f"âœ… å·²åˆªé™¤æ–‡ä»¶: {document_name}\nğŸ—‘ï¸ ç·©å­˜å·²æ¸…ç©º"
            
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                return f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {document_name}\nè«‹ç”¨ rag_list_documents ç¢ºèªæ–‡ä»¶åç¨±"
            return f"âŒ åˆªé™¤å¤±æ•—: {str(e)}"
        except httpx.HTTPError as e:
            return f"âŒ åˆªé™¤å¤±æ•—: {str(e)}"


@mcp.tool()
async def rag_get_status(file_name: str) -> str:
    """
    æŸ¥è©¢æ–‡ä»¶çš„è™•ç†ç‹€æ…‹ï¼ˆä¸Šå‚³å¾Œçš„ç´¢å¼•é€²åº¦ï¼‰ã€‚
    
    Args:
        file_name: æ–‡ä»¶åç¨±
    
    Returns:
        æ–‡ä»¶è™•ç†ç‹€æ…‹å’Œé€²åº¦
    """
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        try:
            response = await client.get(f"{RAG_API_BASE}/status/{file_name}")
            response.raise_for_status()
            status = response.json()
            
            status_emoji = {
                "pending": "â³",
                "processing": "ğŸ”„",
                "completed": "âœ…",
                "failed": "âŒ"
            }
            
            current_status = status.get('status', 'unknown')
            emoji = status_emoji.get(current_status, "â“")
            
            output = [
                f"ğŸ“„ æ–‡ä»¶: {file_name}",
                f"{emoji} ç‹€æ…‹: {current_status}",
                f"ğŸ“Š é€²åº¦: {status.get('progress', 0)}%",
            ]
            
            if status.get('error'):
                output.append(f"âš ï¸ éŒ¯èª¤: {status.get('error')}")
            
            return "\n".join(output)
            
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                return f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_name}"
            return f"âŒ æŸ¥è©¢å¤±æ•—: {str(e)}"
        except httpx.HTTPError as e:
            return f"âŒ æŸ¥è©¢å¤±æ•—: {str(e)}"


# ============================================================
# MCP Tools - ç·©å­˜ç®¡ç†
# ============================================================

@mcp.tool()
async def rag_clear_cache() -> str:
    """
    æ¸…ç©ºæ‰€æœ‰ç·©å­˜ï¼ˆæœå°‹ç·©å­˜ã€å•ç­”ç·©å­˜ã€ç¶²è·¯ç·©å­˜ï¼‰ã€‚
    
    é©åˆç”¨æ–¼ï¼š
    - çŸ¥è­˜åº«å…§å®¹æ›´æ–°å¾Œï¼Œéœ€è¦ç²å–æœ€æ–°çµæœ
    - ç·©å­˜ä½”ç”¨å¤ªå¤šè¨˜æ†¶é«”
    - æ’æŸ¥å•é¡Œæ™‚ç¢ºä¿ç²å–å³æ™‚çµæœ
    
    Returns:
        æ¸…ç©ºçµæœ
    """
    search_before = search_cache.stats()['size']
    ask_before = ask_cache.stats()['size']
    web_before = web_cache.stats()['size']
    
    search_cache.clear()
    ask_cache.clear()
    web_cache.clear()
    
    return (
        f"ğŸ—‘ï¸ ç·©å­˜å·²æ¸…ç©º\n"
        f"   æœå°‹ç·©å­˜: {search_before} â†’ 0\n"
        f"   å•ç­”ç·©å­˜: {ask_before} â†’ 0\n"
        f"   ç¶²è·¯ç·©å­˜: {web_before} â†’ 0"
    )


# ============================================================
# å•Ÿå‹• Server
# ============================================================

if __name__ == "__main__":
    mcp.run()