import logging
import sys
import os
from pathlib import Path
from typing import List, Dict
from dotenv import load_dotenv

# è·¯å¾‘ä¿®æ­£ (é˜²æ­¢ ModuleNotFoundError)
BASE_DIR = Path(__file__).resolve().parents[2]
sys.path.append(str(BASE_DIR))

from llama_index.llms.openai import OpenAI
from llama_index.core.prompts import PromptTemplate

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

logger = logging.getLogger(__name__)

# --- åš´æ ¼çš„ System Prompt ---
# --- ä¿®æ­£å¾Œçš„ System Prompt (å®Œå…¨é˜²å‘†ç‰ˆ) ---
QA_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ä¼æ¥­çŸ¥è­˜åŠ©ç†ã€‚è«‹æ ¹æ“šä¸‹æ–¹çš„ã€åƒè€ƒè³‡æ–™ã€‘å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

å›ç­”è¦å‰‡ï¼š
1. **å¿…é ˆåŸºæ–¼äº‹å¯¦**ï¼šåªèƒ½ä½¿ç”¨æä¾›çš„åƒè€ƒè³‡æ–™å›ç­”ï¼Œä¸è¦ç·¨é€ ã€‚
2. **æ¨™è¨»å¼•ç”¨ä¾†æº**ï¼š
   - æ¯ç•¶ä½ å¼•ç”¨æŸå€‹ç‰‡æ®µçš„è³‡è¨Šæ™‚ï¼Œè«‹æŸ¥çœ‹è©²ç‰‡æ®µé–‹é ­çš„ `ä¾†æº: ...` æ¬„ä½ã€‚
   - åš´æ ¼ä¾ç…§è©²æ¬„ä½çš„å…§å®¹æ¨™è¨»ï¼Œæ ¼å¼ç‚º `[æª”æ¡ˆåç¨±, Page é ç¢¼]`ã€‚
   - ä¸è¦è‡ªå·±ç™¼æ˜æª”åï¼Œ**ç›´æ¥è¤‡è£½**åƒè€ƒè³‡æ–™ä¸­é¡¯ç¤ºçš„æª”åã€‚
3. **çµæ§‹æ¸…æ™°**ï¼šä½¿ç”¨æ¢åˆ—å¼æˆ–æ®µè½åˆ†æ˜çš„æ–¹å¼å›ç­”ã€‚
4. **ç¹é«”ä¸­æ–‡**ï¼šè«‹ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡å›ç­”ã€‚

ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
---------------------
{context_str}
---------------------
"""

class RAGGenerator:
    def __init__(self):
        # ä½¿ç”¨ GPT-4o ç¢ºä¿é‚è¼¯èˆ‡å¼•ç”¨æº–ç¢ºæ€§ 
        self.llm = OpenAI(model="gpt-4o", temperature=0.1)
        self.prompt_tmpl = PromptTemplate(QA_SYSTEM_PROMPT)

    def format_context(self, search_results: List) -> str:
        """
        å°‡ Qdrant çš„æœå°‹çµæœè½‰æ›ç‚ºç´”æ–‡å­— Context 
        """
        context_list = []
        for i, hit in enumerate(search_results):
            payload = hit.payload
            # çµ„è£æ¯ä¸€å€‹ç‰‡æ®µ (Chunk)
            # æˆ‘å€‘æŠŠæ‘˜è¦å’ŒåŸæ–‡éƒ½é¤µçµ¦ LLMï¼Œè®“å®ƒè‡ªå·±åˆ¤æ–·ç´°ç¯€
            chunk_text = (
                f"--- æ–‡ä»¶ç‰‡æ®µ {i+1} ---\n"
                f"ä¾†æº: {payload.get('file_name')}, Page {payload.get('page_label')}\n"
                f"æ‘˜è¦: {payload.get('summary')}\n"
                f"å…§æ–‡: {payload.get('text')}\n"
            )
            context_list.append(chunk_text)
        
        # ç”¨æ›è¡Œæ¥èµ·ä¾†
        return "\n\n".join(context_list)

    def generate(self, query: str, search_results: List) -> str:
        """
        æ ¸å¿ƒç”Ÿæˆé‚è¼¯ï¼šContext + Query -> LLM -> Answer
        """
        if not search_results:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚"

        # 1. æº–å‚™ Context
        context_str = self.format_context(search_results)
        
        # 2. æ ¼å¼åŒ– Prompt
        prompt = self.prompt_tmpl.format(context_str=context_str)
        
        logger.info(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­” (Context size: {len(context_str)} chars)...")
        
        # 3. å‘¼å« LLM
        # æ³¨æ„ï¼šé€™è£¡æ˜¯æŠŠ Prompt å’Œ User Query æ¥åœ¨ä¸€èµ·
        response = self.llm.complete(prompt + f"\n\nä½¿ç”¨è€…å•é¡Œï¼š{query}")
        
        return str(response)

# å–®å…ƒæ¸¬è©¦ (End-to-End Test)
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    # é€™è£¡æœƒç”¨åˆ°æˆ‘å€‘å‰›å¯«å¥½çš„ HybridRetriever
    from src.retrieval.search import HybridRetriever
    
    # 1. å®šç¾©å•é¡Œ
    test_query = "CLIP æ¨¡å‹æ˜¯å¦‚ä½•è¨“ç·´çš„ï¼Ÿ"
    print(f"â“ å•é¡Œ: {test_query}")

    # 2. åŸ·è¡Œæª¢ç´¢ (Phase 2)
    print("ğŸ” Step 1: æª¢ç´¢ä¸­...")
    retriever = HybridRetriever()
    results = retriever.search(test_query, top_k=3)
    
    # 3. åŸ·è¡Œç”Ÿæˆ (Phase 3)
    print("ğŸ¤– Step 2: ç”Ÿæˆä¸­...")
    generator = RAGGenerator()
    answer = generator.generate(test_query, results)
    
    print("\n" + "="*50)
    print("ğŸ’¡ æœ€çµ‚å›ç­” (Final Answer):")
    print("="*50)
    print(answer)
    print("="*50)