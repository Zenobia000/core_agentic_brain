# ===================================================================
# API Keys Management
# ===================================================================
# For security reasons, API keys should be stored in a .env file
# Create a .env file in the OpenManus directory with the following format:
#
# TAVILY_API_KEY=your_tavily_api_key_here
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# GOOGLE_API_KEY=your_google_api_key_here
# BING_API_KEY=your_bing_api_key_here
# DAYTONA_API_KEY=your_daytona_api_key_here
#
# The .env file is automatically loaded when the application starts.
# Never commit your .env file to version control!
# ===================================================================

# Global LLM configuration
[llm]
model = "claude-sonnet-4-5-20250929"       # The LLM model to use
base_url = "https://api.anthropic.com/v1/" # API endpoint URL
api_key = "YOUR_ANTHROPIC_API_KEY"         # Your API key (or use ANTHROPIC_API_KEY in .env file)
max_tokens = 8192                          # Maximum number of tokens in the response
temperature = 0.0                          # Controls randomness

# [llm] # Amazon Bedrock
# api_type = "aws"                                       # Required
# model = "us.anthropic.claude-3-7-sonnet-20250219-v1:0" # Bedrock supported modelID
# base_url = "bedrock-runtime.us-west-2.amazonaws.com"   # Not used now
# max_tokens = 8192
# temperature = 1.0
# api_key = "bear"                                       # Required but not used for Bedrock

# [llm] #AZURE OPENAI:
# api_type= 'azure'
# model = "YOUR_MODEL_NAME" #"gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPLOYMENT_ID}"
# api_key = "AZURE API KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version="AZURE API VERSION" #"2024-08-01-preview"

# [llm] #OLLAMA:
# api_type = 'ollama'
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# [llm] #Jiekou.AI:
# api_type = 'jiekou'
# model = "claude-sonnet-4-5-20250929"                               # The LLM model to use
# base_url = "https://api.jiekou.ai/openai"                          # API endpoint URL
# api_key = "your Jiekou.AI api key"                                 # Your API key
# max_tokens = 64000                                                 # Maximum number of tokens in the response
# temperature = 0.0                                                  # Controls randomness

# [llm] #GOOGLE GEMINI:
# api_type = 'google'
# model = "gemini-2.0-flash"
# base_url = "https://generativelanguage.googleapis.com/v1beta/openai/"
# api_key = "YOUR_API_KEY"
# max_tokens = 8096
# temperature = 0.0

# Optional configuration for specific LLM models
[llm.vision]
model = "claude-sonnet-4-5-20250929"       # The LLM model to use
base_url = "https://api.anthropic.com/v1/" # API endpoint URL
api_key = "YOUR_ANTHROPIC_API_KEY"         # Your API key (or use ANTHROPIC_API_KEY in .env file)
max_tokens = 8192                          # Maximum number of tokens in the response
temperature = 0.0                          # Controls randomness

# [llm.vision] #OLLAMA VISION:
# api_type = 'ollama'
# model = "llama3.2-vision"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# Optional configuration for specific browser configuration
# [browser]
# Whether to run browser in headless mode (default: false)
#headless = false
# Disable browser security features (default: true)
#disable_security = true
# Extra arguments to pass to the browser
#extra_chromium_args = []
# Path to a Chrome instance to use to connect to your normal browser
# e.g. '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
#chrome_instance_path = ""
# Connect to a browser instance via WebSocket
#wss_url = ""
# Connect to a browser instance via CDP
#cdp_url = ""

# Optional configuration, Proxy settings for the browser
# [browser.proxy]
# server = "http://proxy-server:port"
# username = "proxy-username"
# password = "proxy-password"

# Optional configuration, Search settings.
[search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo" or "Bing" or "Tavily".
# Note: For Tavily, you must set TAVILY_API_KEY in your .env file
engine = "Google"
# Fallback engine order. Default is ["DuckDuckGo", "Baidu", "Bing"] - will try in this order after primary engine fails.
fallback_engines = ["Tavily", "DuckDuckGo", "Bing"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
max_retries = 3
# Language code for search results. Options: "en" (English), "zh" (Chinese), etc.
lang = "en"
# Country code for search results. Options: "us" (United States), "cn" (China), etc.
country = "us"


## Sandbox configuration - Docker sandbox (recommended)
[sandbox]
use_sandbox = true                  # Enable Docker sandbox for secure code execution
image = "python:3.12-slim"          # Docker image to use
work_dir = "/workspace"              # Working directory inside container
memory_limit = "1g"                 # Memory limit (can be 512m, 1g, 2g, etc.)
cpu_limit = 2.0                      # CPU cores limit
timeout = 300                        # Execution timeout in seconds
network_enabled = true               # Allow network access from sandbox

# MCP (Model Context Protocol) configuration
[mcp]
server_reference = "app.mcp.server" # default server module reference

# Optional Runflow configuration
# Your can add additional agents into run-flow workflow to solve different-type tasks.
[runflow]
use_data_analysis_agent = true     # The Data Analysi Agent to solve various data analysis tasks

# Daytona configuration (optional - alternative to Docker sandbox)
# Note: If using Docker sandbox above, you don't need Daytona
# [daytona]
# daytona_api_key = "YOUR_DAYTONA_API_KEY"  # Your API key (or use DAYTONA_API_KEY in .env file)